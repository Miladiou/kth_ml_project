\section{Method}
%TODO:
RVMs can be thought of as SVMs that use a Bayesian treatment. Instead of providing direct answers, RVMs return probabilistic predictions. We start of by computing the hyperparameters $\boldsymbol{\alpha},\sigma^2$ of the priors for each weight (all of the priors being independent) based on the data. The roots of the derivatives or the Expectation-maximization (EM) method can be used to find optimal values of $\alpha,\sigma^2$ (i.e. those values that maximize the \textit{evidence}). This is what constitutes the learning process. %Inference is then computed by


%in SVM, support vectors are close to the decision boundary whereas the relevance vectors describe prototypicla examples of classes.
\subsection{Terminology}
\begin{itemize}
	\item $\boldsymbol{x}$ -- the input vector, consisting of $(x_1,\dots x_N)$.
	\item $\boldsymbol{X}$ -- the collection of input vectors such that the $n$th row is $x_n^T$.
	\item $\boldsymbol{t}$ -- the target values vector.
	\item $\boldsymbol{w}$ -- the set of weights, consisting of $(w_0,\dots w_N)$.
\end{itemize}

\subsection{Regression}
Regression with RVMs consists of three main steps: initialization, learning and prediction.
\subsubsection{Assigning a prior to each weight (initialization)} Priors control the importance of a given basis function. The first step is to assign a prior to each weight. Note, the hyperparameters $\alpha_i$ of each weight are independent of each other \footnote{In contrast to SVMs, where a single shared hyperparameter is used [2].}. The parameter $\boldsymbol{\alpha^{-1}}$ is one of the hyperparameters that we aim to optimize later on. The other hyperparameter being the noise variance $\sigma^2$.

The prior of weight $w_i$ is of the form
\begin{equation}
p(w_i|\alpha_i)=\mathcal{N}(w_i|0,\alpha_{i}^{-1})
\end{equation}
When we have multiple data points, the prior $p(\boldsymbol{w}|\boldsymbol{\alpha})$ is simply the product of all the priors of each weight.

\subsubsection{Optimizing the hyperparameters (learning)} In order to keep updating our belief about the weights, we need to compute the posterior distribution. Since both the likelihood and the prior are Gaussian, the resulting posterior
\begin{equation}
p(\boldsymbol{w}|\boldsymbol{t}, \boldsymbol{\alpha}, \sigma^2)=\mathcal{N}(m,\Sigma)
\end{equation}
is also Gaussian, whose parameters (i.e. $m$ and $\Sigma$) can be found given existing formulas [2].

The marginal likelihood function $p(\boldsymbol{t}| \boldsymbol{\alpha}, \sigma^2)$ can be obtained by integrating out the weights. However, the problem becomes untractable if we want to integrate out the hyperparameters in the same fashion after having defined some hyperpriors over them. Thus a less Bayesian approach has to be adopted here.

The aim is therefore to estimate $\boldsymbol{\alpha}$ and $\sigma^2$ that maximize the marginal likelihood function above \footnote{This is known as type-2 maximum likelihood [2].}, which can be accomplished in two ways. Either we can set the derivative of the log marginal likelihood to zero, in order to obtain $\boldsymbol{\alpha}$ and $\sigma^2$, or use an expectation maximization method [1,2]. The new estimation of the hyperparameters can then be used to estimate the mean and the covariance of the posterior [2]. The re-estimation process repeats until convergence \footnote{When an appropriate convergence criterion is satisfied [1]. } [1].

\subsubsection{Prediction}
Once the optimal values of $\boldsymbol{\alpha}$ and $\sigma^2$ have been found, we can use them to find the predictive distribution over $t$ [2]. Given a new input $x$, the predictive distribution $p(t|\boldsymbol{x}, \boldsymbol{X}, \boldsymbol{t}, \boldsymbol{\alpha}, \sigma^2)$ is also a Gaussian, so there exists a closed-form formula to compute the mean and the covariance [2].

\subsection{Classification}
Classification is very similar to regression, with the differences outlined below. Method wise it is the same as described in regression unless stated otherwise.

\subsubsection{Two-class problem}
In order to classify input values to discrete classes, the logistic sigmoid function can be used [2]. It has the property that its range is $[0,1]$.

\subsubsection{Approximating marginal likelihood}
In contrast to regression, it is not possible to integrate out the weight parameter. Instead, Laplacian approximation can be used.


TODO: we can estimate the weights.
