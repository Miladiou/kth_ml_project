\section{Method}
%TODO:
RVMs can be thought of as SVMs that use a Bayesian treatment. Instead of providing direct answers, RVMs return probabilistic predictions. We start of by computing the hyperparameters $\alpha,\sigma^2$ of the priors for each weight (all of the priors being independent) based on the data. Expectation-maximization (EM) method is used to  find optimal values of $\alpha,\sigma^2$ (i.e. those values that maximize the \textit{evidence}). This is what constitutes the learning process. %Inference is then computed by 

%in SVM, support vectors are close to the decision boundary whereas the relevance vectors describe prototypicla examples of classes.

\subsection{Regression}
Regression with RVMs consists of three main steps: initialization, learning and inference. 
\subsubsection{Assigning a prior to each weight (initialization)} Priors control the importance of a given basis function. The first step is to assign a prior to each weight. Note, the hyperparameters of each weight are independent of each other \footnote{In contrast to SVMs, where a single shared hyperparameter is used [2].}. The parameter $\alpha^{-1}$ is one of the parameters that we aim to optimize later on. The other parameter is the mean $\mu$, although it is set to zero in this step.

The prior of weight $i$ is of the form $p(w_i|\alpha_i)=\mathcal{N}(0,\alpha_{i}^{-1})$. When we have multiple data points, the prior $p(\boldsymbol{w}|\boldsymbol{\alpha})$ is simply the product of all the priors of each weight.

\subsubsection{Optimizing the hyperparameters (learning)}

% mention posterior etc.

\subsection{Classification}